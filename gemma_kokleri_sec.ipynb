{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekler = {\n",
    "    \"lak\": 0,\n",
    "    \"nın\": 1,\n",
    "    \"siniz\": 2,\n",
    "    \"ış\": 3,\n",
    "    \"ala\": 4,\n",
    "    \"mış\": 5,\n",
    "    \"ler\": 6,\n",
    "    \"de\": 7,\n",
    "    \"deki\": 8,\n",
    "    \"ka\": 9,\n",
    "    \"dür\": 10,\n",
    "    \"amak\": 11,\n",
    "    \"el\": 12,\n",
    "    \"cuk\": 13,\n",
    "    \"ır\": 14,\n",
    "    \"deş\": 15,\n",
    "    \"gıç\": 16,\n",
    "    \"teş\": 17,\n",
    "    \"eç\": 18,\n",
    "    \"ça\": 19,\n",
    "    \"ma\": 20,\n",
    "    \"imiz\": 21,\n",
    "    \"çıl\": 22,\n",
    "    \"enek\": 23,\n",
    "    \"gaç\": 24,\n",
    "    \"guç\": 25,\n",
    "    \"şer\": 26,\n",
    "    \"ga\": 27,\n",
    "    \"müş\": 28,\n",
    "    \"mek\": 29,\n",
    "    \"layın\": 30,\n",
    "    \"dır\": 31,\n",
    "    \"tan\": 32,\n",
    "    \"li\": 33,\n",
    "    \"ıntı\": 34,\n",
    "    \"anak\": 35,\n",
    "    \"y\": 36,\n",
    "    \"tı\": 37,\n",
    "    \"le\": 38,\n",
    "    \"mede\": 39,\n",
    "    \"nün\": 40,\n",
    "    \"kan\": 41,\n",
    "    \"yecek\": 42,\n",
    "    \"lü\": 43,\n",
    "    \"tır\": 44,\n",
    "    \"cul\": 45,\n",
    "    \"rek\": 46,\n",
    "    \"suz\": 47,\n",
    "    \"ncu\": 48,\n",
    "    \"kıl\": 49,\n",
    "    \"ecek\": 50,\n",
    "    \"mekte\": 51,\n",
    "    \"cik\": 52,\n",
    "    \"ce\": 53,\n",
    "    \"kur\": 54,\n",
    "    \"acak\": 55,\n",
    "    \"tür\": 56,\n",
    "    \"mık\": 57,\n",
    "    \"muk\": 58,\n",
    "    \"mu\": 59,\n",
    "    \"gül\": 60,\n",
    "    \"te\": 61,\n",
    "    \"çü\": 62,\n",
    "    \"meç\": 63,\n",
    "    \"nci\": 64,\n",
    "    \"sel\": 65,\n",
    "    \"ydi\": 66,\n",
    "    \"şin\": 67,\n",
    "    \"rak\": 68,\n",
    "    \"tü\": 69,\n",
    "    \"me\": 70,\n",
    "    \"ti\": 71,\n",
    "    \"leyin\": 72,\n",
    "    \"sı\": 73,\n",
    "    \"sal\": 74,\n",
    "    \"tu\": 75,\n",
    "    \"muş\": 76,\n",
    "    \"leri\": 77,\n",
    "    \"gı\": 78,\n",
    "    \"cü\": 79,\n",
    "    \"ek\": 80,\n",
    "    \"ar\": 81,\n",
    "    \"eceğ\": 82,\n",
    "    \"çuk\": 83,\n",
    "    \"ak\": 84,\n",
    "    \"dü\": 85,\n",
    "    \"den\": 86,\n",
    "    \"çı\": 87,\n",
    "    \"an\": 88,\n",
    "    \"alak\": 89,\n",
    "    \"ncı\": 90,\n",
    "    \"kür\": 91,\n",
    "    \"cu\": 92,\n",
    "    \"mi\": 93,\n",
    "    \"dir\": 94,\n",
    "    \"aç\": 95,\n",
    "    \"t\": 96,\n",
    "    \"cı\": 97,\n",
    "    \"geç\": 98,\n",
    "    \"cıl\": 99,\n",
    "    \"ca\": 100,\n",
    "    \"iniz\": 101,\n",
    "    \"miş\": 102,\n",
    "    \"ı\": 103,\n",
    "    \"da\": 104,\n",
    "    \"k\": 105,\n",
    "    \"ün\": 106,\n",
    "    \"mik\": 107,\n",
    "    \"l\": 108,\n",
    "    \"al\": 109,\n",
    "    \"çük\": 110,\n",
    "    \"luk\": 111,\n",
    "    \"dan\": 112,\n",
    "    \"dirler\": 113,\n",
    "    \"lik\": 114,\n",
    "    \"sek\": 115,\n",
    "    \"iz\": 116,\n",
    "    \"kir\": 117,\n",
    "    \"yse\": 118,\n",
    "    \"p\": 119,\n",
    "    \"cül\": 120,\n",
    "    \"cileyin\": 121,\n",
    "    \"mü\": 122,\n",
    "    \"taş\": 123,\n",
    "    \"çil\": 124,\n",
    "    \"u\": 125,\n",
    "    \"ımsa\": 126,\n",
    "    \"kır\": 127,\n",
    "    \"si\": 128,\n",
    "    \"r\": 129,\n",
    "    \"n\": 130,\n",
    "    \"sü\": 131,\n",
    "    \"lı\": 132,\n",
    "    \"m\": 133,\n",
    "    \"makta\": 134,\n",
    "    \"man\": 135,\n",
    "    \"üncü\": 136,\n",
    "    \"nç\": 137,\n",
    "    \"lık\": 138,\n",
    "    \"dı\": 139,\n",
    "    \"la\": 140,\n",
    "    \"mece\": 141,\n",
    "    \"ten\": 142,\n",
    "    \"sin\": 143,\n",
    "    \"ele\": 144,\n",
    "    \"ın\": 145,\n",
    "    \"sa\": 146,\n",
    "    \"ayım\": 147,\n",
    "    \"çi\": 148,\n",
    "    \"eğen\": 149,\n",
    "    \"ge\": 150,\n",
    "    \"msi\": 151,\n",
    "    \"tur\": 152,\n",
    "    \"ken\": 153,\n",
    "    \"lek\": 154,\n",
    "    \"mse\": 155,\n",
    "    \"lar\": 156,\n",
    "    \"tir\": 157,\n",
    "    \"malı\": 158,\n",
    "    \"lu\": 159,\n",
    "    \"sız\": 160,\n",
    "    \"i\": 161,\n",
    "    \"süz\": 162,\n",
    "    \"se\": 163,\n",
    "    \"msı\": 164,\n",
    "    \"a\": 165,\n",
    "    \"mtırak\": 166,\n",
    "    \"daş\": 167,\n",
    "    \"ncü\": 168,\n",
    "    \"msu\": 169,\n",
    "    \"e\": 170,\n",
    "    \"çık\": 171,\n",
    "    \"men\": 172,\n",
    "    \"ç\": 173,\n",
    "    \"meli\": 174,\n",
    "    \"gil\": 175,\n",
    "    \"çul\": 176,\n",
    "    \"cil\": 177,\n",
    "    \"emek\": 178,\n",
    "    \"maca\": 179,\n",
    "    \"z\": 180,\n",
    "    \"sak\": 181,\n",
    "    \"du\": 182,\n",
    "    \"acağ\": 183,\n",
    "    \"kil\": 184,\n",
    "    \"di\": 185,\n",
    "    \"ş\": 186,\n",
    "    \"msa\": 187,\n",
    "    \"en\": 188,\n",
    "    \"ağan\": 189,\n",
    "    \"mak\": 190,\n",
    "    \"ıcı\": 191,\n",
    "    \"siz\": 192,\n",
    "    \"şar\": 193,\n",
    "    \"elek\": 194,\n",
    "    \"giç\": 195,\n",
    "    \"ki\": 196,\n",
    "    \"yacak\": 197,\n",
    "    \"çe\": 198,\n",
    "    \"şın\": 199,\n",
    "    \"kaç\": 200,\n",
    "    \"maç\": 201,\n",
    "    \"in\": 202,\n",
    "    \"ü\": 203,\n",
    "    \"nin\": 204,\n",
    "    \"ık\": 205,\n",
    "    \"kek\": 206,\n",
    "    \"cük\": 207,\n",
    "    \"im\": 208,\n",
    "    \"laş\": 209,\n",
    "    \"yor\": 210,\n",
    "    \"mı\": 211,\n",
    "    \"ım\": 212,\n",
    "    \"su\": 213,\n",
    "    \"lük\": 214,\n",
    "    \"çu\": 215,\n",
    "    \"dur\": 216,\n",
    "    \"un\": 217,\n",
    "    \"ci\": 218,\n",
    "    \"ymiş\": 219,\n",
    "    \"er\": 220,\n",
    "    \"cık\": 221,\n",
    "    \"çik\": 222,\n",
    "    \"ta\": 223,\n",
    "    \"nun\": 224\n",
    "}\n",
    "\n",
    "ekler = ekler.keys()\n",
    "\n",
    "len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TURKISH_SUFFIXES = [\n",
    "    \"lar\", \"ler\",\n",
    "    # Common case endings\n",
    "    \"ı\", \"i\", \"u\", \"ü\",\n",
    "    \"yı\", \"yi\", \"yu\", \"yü\",\n",
    "    \"a\", \"e\",\n",
    "    \"ya\", \"ye\",\n",
    "    \"da\", \"de\", \"ta\", \"te\",\n",
    "    \"dan\", \"den\", \"tan\", \"ten\",\n",
    "    # Possessive\n",
    "    \"ım\", \"im\", \"um\", \"üm\",\n",
    "    \"nın\", \"nin\", \"nun\", \"nün\",\n",
    "    \"ımız\", \"imiz\", \"umuz\", \"ümüz\",\n",
    "    \"nız\", \"niz\", \"nuz\", \"nüz\",\n",
    "    # Verb tenses / person\n",
    "    \"m\", \"n\", \"k\", \"ız\", \"iz\", \"uz\", \"üz\",\n",
    "    \"dı\", \"di\", \"du\", \"dü\", \"tı\", \"ti\", \"tu\", \"tü\",\n",
    "    \"mış\", \"miş\", \"muş\", \"müş\",\n",
    "    \"acak\", \"ecek\",\n",
    "    \"ar\", \"er\",\n",
    "    # Some derivational\n",
    "    \"lık\", \"lik\", \"luk\", \"lük\",\n",
    "    \"cı\", \"ci\", \"cu\", \"cü\",\n",
    "    \"çı\", \"çi\", \"çu\", \"çü\",\n",
    "    \"laş\", \"leş\", \"laşma\", \"leşme\",\n",
    "    \"ma\", \"me\", \"ış\", \"iş\", \"uş\", \"üş\",\n",
    "    # ... add more if needed\n",
    "]\n",
    "\n",
    "TURKISH_SUFFIXES = set(TURKISH_SUFFIXES)\n",
    "for ek in ekler:\n",
    "    TURKISH_SUFFIXES.add(ek)\n",
    "\n",
    "len(TURKISH_SUFFIXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "# Elinizdeki kelime frekans listesinin JSON formatındaki örneği\n",
    "with open(\"sorted_combined_remains_words_with_freq.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_frequencies = json.load(f)\n",
    "\n",
    "# Sadece kelimeleri almak\n",
    "words = list(word_frequencies.keys())\n",
    "\n",
    "# 100'er gruplara bölme\n",
    "def chunk_list(data, chunk_size=100):\n",
    "    \"\"\"Bir listeyi belirtilen boyutta parçalara böler.\"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# Ollama'dan kökleri alma\n",
    "def get_roots_from_ollama(words_chunk, suffixes):\n",
    "    # Sistem promptu\n",
    "    system_prompt = f\"\"\"\n",
    "    Sen bir Türkçe kök analizi yapan uzman bir araçsın. Görevin, verilen Türkçe kelimelerin köklerini bulmaktır. \n",
    "\n",
    "    1. Yapım ve çekim eklerini çıkararak kelimenin kökünü yaz.\n",
    "    2. Sadece Türkçe olan kelimeleri işle. Türkçe'de yaygın kullanılan yabancı kökenli kelimeleri (ör. \"tren\") Türkçe sayabilirsin.\n",
    "    3. Kısaltmaları olduğu gibi bırak. Uzun hallerini yazma.\n",
    "    4. Türkçe olmayan kelimeleri sonuçlara dahil etme.\n",
    "    5. Çıktıyı yalnızca JSON formatında bir liste olarak ver. Örnek:\n",
    "    [\n",
    "      \"kök1\",\n",
    "      \"kök2\",\n",
    "      \"kök3\"\n",
    "    ]\n",
    "\n",
    "    Türkçe'deki yapım ve çekim ekleri şunlardır:\n",
    "    {suffixes}\n",
    "\n",
    "    Aşağıdaki kelimelerin köklerini bul ve yalnızca JSON formatında bir liste döndür. Eğer kök bulunamazsa boş liste döndür:\n",
    "    []\n",
    "    \"\"\"\n",
    "\n",
    "    # Kullanıcı promptu\n",
    "    user_prompt = (\n",
    "        f\"Aşağıdaki kelimelerin Türkçe köklerini bul:\\n\\n\"\n",
    "        f\"{', '.join(words_chunk)}\\n\\n\"\n",
    "        f\"Lütfen başka bir şey yazma ve sadece JSON formatında cevap ver.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "         # Ollama'ya sorgu gönder\n",
    "        response = chat(model=\"gemma2:27b\", messages=[\n",
    "            {'role': 'system', 'content': system_prompt},  # Sistem promptu\n",
    "            {'role': 'user', 'content': user_prompt},      # Kullanıcı promptu\n",
    "        ])\n",
    "\n",
    "        # Dönen yanıt\n",
    "        return response.message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "        return None\n",
    "\n",
    "chunks = tqdm(chunk_list(words, chunk_size=200), desc=\"Creating chunks\")\n",
    "# Tüm kelimeleri 100'erli gruplar halinde işle\n",
    "chunks = list(chunks)\n",
    "counter = 300\n",
    "for chunk in chunks[counter:]:    \n",
    "    print(f\"Processing chunk {counter} in progress...\")\n",
    "    response = get_roots_from_ollama(chunk, TURKISH_SUFFIXES)\n",
    "    if response:\n",
    "        file_name = f\"r_{chunk[0]}_{chunk[-1]}\".replace(\"/\", \"-\")\n",
    "        file_name = f\"roots2/{file_name}.json\"\n",
    "        try:\n",
    "            # JSON yanıtını Python listesine dönüştür\n",
    "            response = json.loads(response)\n",
    "\n",
    "            # Her bir chunk'ın sonuçlarını ayrı bir dosyaya kaydet            \n",
    "            with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(response, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            # write response to a text file\n",
    "            with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response)\n",
    "            print(f\"JSON dönüşümü sırasında hata oluştu: {e}\")\n",
    "        print(f\"Processing chunk {counter} succeeded.\")\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"Processing chunk failed.\")\n",
    "print(\"Tüm kökler başarıyla kaydedildi!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "# Elinizdeki kelime listesini roots_gemma_v1.txt dosyasından okuyun\n",
    "with open(\"combined_roots.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    words = f.readlines()\n",
    "\n",
    "print(f\"Toplam {len(words)} kelime bulundu.\")\n",
    "\n",
    "# 100'er gruplara bölme\n",
    "def chunk_list(data, chunk_size=100):\n",
    "    \"\"\"Bir listeyi belirtilen boyutta parçalara böler.\"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# Ollama'dan kökleri alma\n",
    "def get_roots_from_ollama(words_chunk, suffixes):\n",
    "    # Sistem promptu\n",
    "    # Sistem promptu  \n",
    "    system_prompt = \"\"\"  \n",
    "    Sen bir Türkçe kök analizi yapan uzman bir araçsın. Görevin, verilen kelimelerden yalnızca **Türkçe** olanların köklerini bulmaktır.  \n",
    "\n",
    "    **Kurallar ve Örnekler:**  \n",
    "\n",
    "    1. **Türkçe olmayan kelimeleri asla sonuçlara dahil etme.**   \n",
    "      Dahil edilmeyecek örnekler:  \n",
    "      - \"computer\" → dahil edilmez  \n",
    "      - \"Miami\" → dahil edilmez  \n",
    "      - \"software\" → dahil edilmez  \n",
    "      - \"London\" → dahil edilmez  \n",
    "      - \"pizza\" → dahil edilmez  \n",
    "      - \"coffee\" → dahil edilmez  \n",
    "\n",
    "    2. Yapım ve çekim eklerini çıkararak kelimenin kökünü bul.  \n",
    "      Doğru örnekler:  \n",
    "      - \"çalışıyor\" → \"çalış\"  \n",
    "      - \"kitaplar\" → \"kitap\"  \n",
    "      - \"gözlükçü\" → \"göz\"  \n",
    "      - \"balıkçılık\" → \"balık\"  \n",
    "      - \"evdekiler\" → \"ev\"  \n",
    "      - \"kalemim\" → \"kalem\"  \n",
    "      - \"gösterici\" → \"göster\"  \n",
    "      - \"yazabilir\" → \"yaz\"  \n",
    "      \n",
    "    3. **Türkçe'de çok yaygın kullanılan ve Türkçeleşmiş sayılı yabancı kökenli kelimeleri Türkçe sayabilirsin.**  \n",
    "      Kabul edilebilir örnekler:  \n",
    "      - \"tren\" → \"tren\"  \n",
    "      - \"televizyon\" → \"televizyon\"  \n",
    "      - \"telefon\" → \"telefon\"  \n",
    "      \n",
    "      Kabul edilmeyecek örnekler:  \n",
    "      - \"computer\" → dahil edilmez  \n",
    "      - \"keyboard\" → dahil edilmez  \n",
    "      - \"mouse\" → dahil edilmez  \n",
    "\n",
    "    4. Kısaltmaları olduğu gibi bırak. Uzun hallerini yazma veya değiştirme.  \n",
    "      - \"TBMM\" → \"TBMM\"  \n",
    "      - \"İTÜ\" → \"İTÜ\"  \n",
    "      - \"TDK\" → \"TDK\"  \n",
    "\n",
    "    5. Özel isimleri dahil etme:  \n",
    "      - \"Ahmet\" → dahil edilmez  \n",
    "      - \"İstanbul\" → dahil edilmez  \n",
    "      - \"Facebook\" → dahil edilmez  \n",
    "      - \"Google\" → dahil edilmez  \n",
    "\n",
    "    6. Çıktıyı **yalnızca JSON formatında** bir liste olarak ver. Her kök yalnızca bir kez yer almalıdır.   \n",
    "      Örnek çıktı:  \n",
    "      [  \n",
    "        \"çalış\",  \n",
    "        \"kitap\",  \n",
    "        \"göz\"  \n",
    "      ]  \n",
    "\n",
    "    7. Hiçbir kelime için kök bulunamazsa, boş liste döndür: []  \n",
    "\n",
    "    Ek Kurallar:  \n",
    "    • Türkçe olmayan veya kısaltma niteliğinde olmayan herhangi bir kelimeyi kök bulma sürecine dahil etme.  \n",
    "    • Kelimenin kökü mevcut değilse veya kelime Türkçe değilse, o kelimeyi tamamen yok say.  \n",
    "    • Emin olmadığın durumlarda kelimeyi dahil etme.  \n",
    "\n",
    "    Türkçe'deki Ekler:  \n",
    "    Türkçe yapım ve çekim ekleri şunlardır:  \n",
    "    \"\"\" + ', '.join(suffixes) + \"\"\"  \n",
    "\n",
    "    Aşağıdaki kelimelerin köklerini bul ve yalnızca JSON formatında bir liste döndür. Başka hiçbir şey yazma:  \n",
    "    \"\"\"  \n",
    "\n",
    "    # Kullanıcı promptu\n",
    "    user_prompt = (\n",
    "        f\"Aşağıdaki kelimelerin Türkçe köklerini bul:\\n\\n\"\n",
    "        f\"{', '.join(words_chunk)}\\n\\n\"\n",
    "        f\"Lütfen başka bir şey yazma ve sadece JSON formatında cevap ver.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "         # Ollama'ya sorgu gönder\n",
    "        response = chat(model=\"gemma2:27b\", messages=[\n",
    "            {'role': 'system', 'content': system_prompt},  # Sistem promptu\n",
    "            {'role': 'user', 'content': user_prompt},      # Kullanıcı promptu\n",
    "        ])\n",
    "\n",
    "        # Dönen yanıt\n",
    "        return response.message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Tüm kelimeleri gruplar halinde işle\n",
    "chunks = tqdm(chunk_list(words, chunk_size=25), desc=\"Creating chunks\")\n",
    "chunks = list(chunks)\n",
    "counter = 0\n",
    "for chunk in chunks[counter:]:    \n",
    "    print(f\"Processing chunk {counter} in progress...\")\n",
    "    response = get_roots_from_ollama(chunk, TURKISH_SUFFIXES)\n",
    "    if response:\n",
    "        file_name = f\"r_{chunk[0]}_{chunk[-1]}\".replace(\"/\", \"-\")\n",
    "        file_name = f\"roots/{file_name}.json\"\n",
    "        try:\n",
    "            # JSON yanıtını Python listesine dönüştür\n",
    "            response = json.loads(response)\n",
    "\n",
    "            # Her bir chunk'ın sonuçlarını ayrı bir dosyaya kaydet            \n",
    "            with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(response, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            # write response to a text file\n",
    "            with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response)\n",
    "            print(f\"JSON dönüşümü sırasında hata oluştu: {e}\")\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(f\"Processing chunk failed.\")\n",
    "print(\"Tüm kökler başarıyla kaydedildi!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
