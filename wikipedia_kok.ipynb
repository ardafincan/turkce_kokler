{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekler = {\n",
    "    \"lak\": 0,\n",
    "    \"nın\": 1,\n",
    "    \"siniz\": 2,\n",
    "    \"ış\": 3,\n",
    "    \"ala\": 4,\n",
    "    \"mış\": 5,\n",
    "    \"ler\": 6,\n",
    "    \"de\": 7,\n",
    "    \"deki\": 8,\n",
    "    \"ka\": 9,\n",
    "    \"dür\": 10,\n",
    "    \"amak\": 11,\n",
    "    \"el\": 12,\n",
    "    \"cuk\": 13,\n",
    "    \"ır\": 14,\n",
    "    \"deş\": 15,\n",
    "    \"gıç\": 16,\n",
    "    \"teş\": 17,\n",
    "    \"eç\": 18,\n",
    "    \"ça\": 19,\n",
    "    \"ma\": 20,\n",
    "    \"imiz\": 21,\n",
    "    \"çıl\": 22,\n",
    "    \"enek\": 23,\n",
    "    \"gaç\": 24,\n",
    "    \"guç\": 25,\n",
    "    \"şer\": 26,\n",
    "    \"ga\": 27,\n",
    "    \"müş\": 28,\n",
    "    \"mek\": 29,\n",
    "    \"layın\": 30,\n",
    "    \"dır\": 31,\n",
    "    \"tan\": 32,\n",
    "    \"li\": 33,\n",
    "    \"ıntı\": 34,\n",
    "    \"anak\": 35,\n",
    "    \"y\": 36,\n",
    "    \"tı\": 37,\n",
    "    \"le\": 38,\n",
    "    \"mede\": 39,\n",
    "    \"nün\": 40,\n",
    "    \"kan\": 41,\n",
    "    \"yecek\": 42,\n",
    "    \"lü\": 43,\n",
    "    \"tır\": 44,\n",
    "    \"cul\": 45,\n",
    "    \"rek\": 46,\n",
    "    \"suz\": 47,\n",
    "    \"ncu\": 48,\n",
    "    \"kıl\": 49,\n",
    "    \"ecek\": 50,\n",
    "    \"mekte\": 51,\n",
    "    \"cik\": 52,\n",
    "    \"ce\": 53,\n",
    "    \"kur\": 54,\n",
    "    \"acak\": 55,\n",
    "    \"tür\": 56,\n",
    "    \"mık\": 57,\n",
    "    \"muk\": 58,\n",
    "    \"mu\": 59,\n",
    "    \"gül\": 60,\n",
    "    \"te\": 61,\n",
    "    \"çü\": 62,\n",
    "    \"meç\": 63,\n",
    "    \"nci\": 64,\n",
    "    \"sel\": 65,\n",
    "    \"ydi\": 66,\n",
    "    \"şin\": 67,\n",
    "    \"rak\": 68,\n",
    "    \"tü\": 69,\n",
    "    \"me\": 70,\n",
    "    \"ti\": 71,\n",
    "    \"leyin\": 72,\n",
    "    \"sı\": 73,\n",
    "    \"sal\": 74,\n",
    "    \"tu\": 75,\n",
    "    \"muş\": 76,\n",
    "    \"leri\": 77,\n",
    "    \"gı\": 78,\n",
    "    \"cü\": 79,\n",
    "    \"ek\": 80,\n",
    "    \"ar\": 81,\n",
    "    \"eceğ\": 82,\n",
    "    \"çuk\": 83,\n",
    "    \"ak\": 84,\n",
    "    \"dü\": 85,\n",
    "    \"den\": 86,\n",
    "    \"çı\": 87,\n",
    "    \"an\": 88,\n",
    "    \"alak\": 89,\n",
    "    \"ncı\": 90,\n",
    "    \"kür\": 91,\n",
    "    \"cu\": 92,\n",
    "    \"mi\": 93,\n",
    "    \"dir\": 94,\n",
    "    \"aç\": 95,\n",
    "    \"t\": 96,\n",
    "    \"cı\": 97,\n",
    "    \"geç\": 98,\n",
    "    \"cıl\": 99,\n",
    "    \"ca\": 100,\n",
    "    \"iniz\": 101,\n",
    "    \"miş\": 102,\n",
    "    \"ı\": 103,\n",
    "    \"da\": 104,\n",
    "    \"k\": 105,\n",
    "    \"ün\": 106,\n",
    "    \"mik\": 107,\n",
    "    \"l\": 108,\n",
    "    \"al\": 109,\n",
    "    \"çük\": 110,\n",
    "    \"luk\": 111,\n",
    "    \"dan\": 112,\n",
    "    \"dirler\": 113,\n",
    "    \"lik\": 114,\n",
    "    \"sek\": 115,\n",
    "    \"iz\": 116,\n",
    "    \"kir\": 117,\n",
    "    \"yse\": 118,\n",
    "    \"p\": 119,\n",
    "    \"cül\": 120,\n",
    "    \"cileyin\": 121,\n",
    "    \"mü\": 122,\n",
    "    \"taş\": 123,\n",
    "    \"çil\": 124,\n",
    "    \"u\": 125,\n",
    "    \"ımsa\": 126,\n",
    "    \"kır\": 127,\n",
    "    \"si\": 128,\n",
    "    \"r\": 129,\n",
    "    \"n\": 130,\n",
    "    \"sü\": 131,\n",
    "    \"lı\": 132,\n",
    "    \"m\": 133,\n",
    "    \"makta\": 134,\n",
    "    \"man\": 135,\n",
    "    \"üncü\": 136,\n",
    "    \"nç\": 137,\n",
    "    \"lık\": 138,\n",
    "    \"dı\": 139,\n",
    "    \"la\": 140,\n",
    "    \"mece\": 141,\n",
    "    \"ten\": 142,\n",
    "    \"sin\": 143,\n",
    "    \"ele\": 144,\n",
    "    \"ın\": 145,\n",
    "    \"sa\": 146,\n",
    "    \"ayım\": 147,\n",
    "    \"çi\": 148,\n",
    "    \"eğen\": 149,\n",
    "    \"ge\": 150,\n",
    "    \"msi\": 151,\n",
    "    \"tur\": 152,\n",
    "    \"ken\": 153,\n",
    "    \"lek\": 154,\n",
    "    \"mse\": 155,\n",
    "    \"lar\": 156,\n",
    "    \"tir\": 157,\n",
    "    \"malı\": 158,\n",
    "    \"lu\": 159,\n",
    "    \"sız\": 160,\n",
    "    \"i\": 161,\n",
    "    \"süz\": 162,\n",
    "    \"se\": 163,\n",
    "    \"msı\": 164,\n",
    "    \"a\": 165,\n",
    "    \"mtırak\": 166,\n",
    "    \"daş\": 167,\n",
    "    \"ncü\": 168,\n",
    "    \"msu\": 169,\n",
    "    \"e\": 170,\n",
    "    \"çık\": 171,\n",
    "    \"men\": 172,\n",
    "    \"ç\": 173,\n",
    "    \"meli\": 174,\n",
    "    \"gil\": 175,\n",
    "    \"çul\": 176,\n",
    "    \"cil\": 177,\n",
    "    \"emek\": 178,\n",
    "    \"maca\": 179,\n",
    "    \"z\": 180,\n",
    "    \"sak\": 181,\n",
    "    \"du\": 182,\n",
    "    \"acağ\": 183,\n",
    "    \"kil\": 184,\n",
    "    \"di\": 185,\n",
    "    \"ş\": 186,\n",
    "    \"msa\": 187,\n",
    "    \"en\": 188,\n",
    "    \"ağan\": 189,\n",
    "    \"mak\": 190,\n",
    "    \"ıcı\": 191,\n",
    "    \"siz\": 192,\n",
    "    \"şar\": 193,\n",
    "    \"elek\": 194,\n",
    "    \"giç\": 195,\n",
    "    \"ki\": 196,\n",
    "    \"yacak\": 197,\n",
    "    \"çe\": 198,\n",
    "    \"şın\": 199,\n",
    "    \"kaç\": 200,\n",
    "    \"maç\": 201,\n",
    "    \"in\": 202,\n",
    "    \"ü\": 203,\n",
    "    \"nin\": 204,\n",
    "    \"ık\": 205,\n",
    "    \"kek\": 206,\n",
    "    \"cük\": 207,\n",
    "    \"im\": 208,\n",
    "    \"laş\": 209,\n",
    "    \"yor\": 210,\n",
    "    \"mı\": 211,\n",
    "    \"ım\": 212,\n",
    "    \"su\": 213,\n",
    "    \"lük\": 214,\n",
    "    \"çu\": 215,\n",
    "    \"dur\": 216,\n",
    "    \"un\": 217,\n",
    "    \"ci\": 218,\n",
    "    \"ymiş\": 219,\n",
    "    \"er\": 220,\n",
    "    \"cık\": 221,\n",
    "    \"çik\": 222,\n",
    "    \"ta\": 223,\n",
    "    \"nun\": 224\n",
    "}\n",
    "\n",
    "ekler = ekler.keys()\n",
    "\n",
    "len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alibayram/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Cengiz%20Han</td>\n",
       "      <td>Cengiz Han</td>\n",
       "      <td>Cengiz Han (doğum adıyla Temuçin,  – 18 Ağusto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Film%20%28anlam%...</td>\n",
       "      <td>Film (anlam ayrımı)</td>\n",
       "      <td>Film şu anlamlara gelebilir:\\n\\n Camlara yapış...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Mustafa%20Suphi</td>\n",
       "      <td>Mustafa Suphi</td>\n",
       "      <td>Mehmed Mustafa Subhi (), kısaca Mustafa Suphi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Linux</td>\n",
       "      <td>Linux</td>\n",
       "      <td>Linux (telaffuz: Lin-uks); Linux çekirdeğine d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Bol%C5%9Fevizm</td>\n",
       "      <td>Bolşevizm</td>\n",
       "      <td>Bolşevik, çoğunluktan yana anlamına gelen Rusç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534983</th>\n",
       "      <td>3611624</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Musculus%20ptery...</td>\n",
       "      <td>Musculus pterygoideus lateralis</td>\n",
       "      <td>ağzı kapatan tek kastır. alt çene ramus'a ve ü...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534984</th>\n",
       "      <td>3611625</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/%C3%87in%20kad%C...</td>\n",
       "      <td>Çin kadın millî futbol takımı</td>\n",
       "      <td>Çin kadın millî futbol takımı, Çin'i uluslarar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534985</th>\n",
       "      <td>3611628</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Abdurrezak%20KUR...</td>\n",
       "      <td>Abdurrezak KURTULUŞ</td>\n",
       "      <td>1933 yılında Mardin’de doğdu. 1950 yılında Diy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534986</th>\n",
       "      <td>3611631</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/%C4%B0brahim%20T...</td>\n",
       "      <td>İbrahim Turgut</td>\n",
       "      <td>İbrahim Turgut Sayın Cumhurbaşkanı Recep Tayyi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534987</th>\n",
       "      <td>3611637</td>\n",
       "      <td>https://tr.wikipedia.org/wiki/Burcu%20Pirin%C3...</td>\n",
       "      <td>Burcu Pirinçci</td>\n",
       "      <td>Burcu Dindar (evlilik öncesi Pirinçci) (d. 8 Ş...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534988 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                                url  \\\n",
       "0            10         https://tr.wikipedia.org/wiki/Cengiz%20Han   \n",
       "1            16  https://tr.wikipedia.org/wiki/Film%20%28anlam%...   \n",
       "2            22      https://tr.wikipedia.org/wiki/Mustafa%20Suphi   \n",
       "3            24                https://tr.wikipedia.org/wiki/Linux   \n",
       "4            30       https://tr.wikipedia.org/wiki/Bol%C5%9Fevizm   \n",
       "...         ...                                                ...   \n",
       "534983  3611624  https://tr.wikipedia.org/wiki/Musculus%20ptery...   \n",
       "534984  3611625  https://tr.wikipedia.org/wiki/%C3%87in%20kad%C...   \n",
       "534985  3611628  https://tr.wikipedia.org/wiki/Abdurrezak%20KUR...   \n",
       "534986  3611631  https://tr.wikipedia.org/wiki/%C4%B0brahim%20T...   \n",
       "534987  3611637  https://tr.wikipedia.org/wiki/Burcu%20Pirin%C3...   \n",
       "\n",
       "                                  title  \\\n",
       "0                            Cengiz Han   \n",
       "1                   Film (anlam ayrımı)   \n",
       "2                         Mustafa Suphi   \n",
       "3                                 Linux   \n",
       "4                             Bolşevizm   \n",
       "...                                 ...   \n",
       "534983  Musculus pterygoideus lateralis   \n",
       "534984    Çin kadın millî futbol takımı   \n",
       "534985              Abdurrezak KURTULUŞ   \n",
       "534986                   İbrahim Turgut   \n",
       "534987                   Burcu Pirinçci   \n",
       "\n",
       "                                                     text  \n",
       "0       Cengiz Han (doğum adıyla Temuçin,  – 18 Ağusto...  \n",
       "1       Film şu anlamlara gelebilir:\\n\\n Camlara yapış...  \n",
       "2       Mehmed Mustafa Subhi (), kısaca Mustafa Suphi,...  \n",
       "3       Linux (telaffuz: Lin-uks); Linux çekirdeğine d...  \n",
       "4       Bolşevik, çoğunluktan yana anlamına gelen Rusç...  \n",
       "...                                                   ...  \n",
       "534983  ağzı kapatan tek kastır. alt çene ramus'a ve ü...  \n",
       "534984  Çin kadın millî futbol takımı, Çin'i uluslarar...  \n",
       "534985  1933 yılında Mardin’de doğdu. 1950 yılında Diy...  \n",
       "534986  İbrahim Turgut Sayın Cumhurbaşkanı Recep Tayyi...  \n",
       "534987  Burcu Dindar (evlilik öncesi Pirinçci) (d. 8 Ş...  \n",
       "\n",
       "[534988 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.tr\")\n",
    "df = ds['train'].to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534988"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_with_title = df['title'] + \" \" + df['text']\n",
    "texts = texts_with_title.to_list()\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_suffixes(word, suffixes, known_roots, memo=None):\n",
    "    \"\"\"\n",
    "    Recursively strip known Turkish suffixes from 'word'.\n",
    "    Returns a set of all possible stripped forms that are in 'known_roots'.\n",
    "\n",
    "    word: str\n",
    "    suffixes: list of suffix strings\n",
    "    known_roots: set of valid root words\n",
    "    memo: dict used for memoization to avoid repeated computation\n",
    "    \"\"\"\n",
    "    if memo is None:\n",
    "        memo = {}\n",
    "    if word in memo:\n",
    "        return memo[word]\n",
    "\n",
    "    candidates = set()\n",
    "    \n",
    "    # If the current 'word' itself is a known root, consider it a valid candidate.\n",
    "    if word in known_roots:\n",
    "        candidates.add(word)\n",
    "\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix) and len(word) > len(suffix):\n",
    "            # Cut off the suffix\n",
    "            truncated = word[:-len(suffix)]\n",
    "\n",
    "            # Recursively strip from the truncated form\n",
    "            sub_candidates = strip_suffixes(truncated, suffixes, known_roots, memo)\n",
    "            candidates.update(sub_candidates)\n",
    "\n",
    "    memo[word] = candidates\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10470 known roots from 'kokler.txt'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "known_roots_file = \"kokler.txt\"\n",
    "output_file      = \"possible_roots.txt\"\n",
    "\n",
    "# Load known roots into a set\n",
    "known_roots = set()\n",
    "with open(known_roots_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        root = line.strip().lower()\n",
    "        if root:\n",
    "            known_roots.add(root)\n",
    "\n",
    "print(f\"Loaded {len(known_roots)} known roots from '{known_roots_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Collect final root candidates\n",
    "all_roots = set()\n",
    "\n",
    "# Simple regex to keep only Turkish letters, ignoring punctuation/numbers\n",
    "word_pattern = re.compile(r\"[abcçdefgğhıijklmnoöprsştuüvyz]+\", re.IGNORECASE)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for line in texts[:100]:\n",
    "    # Lowercase the line to unify everything\n",
    "    line = line.lower()\n",
    "    # Extract words\n",
    "    tokens = word_pattern.findall(line)\n",
    "    for token in tokens:\n",
    "        # Optionally skip very short tokens (like 'a', 'e')\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "        # Strip suffixes, but only keep results that are in the known_roots set\n",
    "        root_candidates = strip_suffixes(token, ekler, known_roots)\n",
    "        all_roots.update(root_candidates)\n",
    "\n",
    "    counter += 1\n",
    "    if counter % 1000 == 0:\n",
    "        print(f\"Processed {counter} lines, found {len(all_roots)} unique roots so far.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Collected 6088 possible roots.\n",
      "Results saved to 'possible_roots.txt'.\n"
     ]
    }
   ],
   "source": [
    "# 3) Write the filtered roots to file\n",
    "sorted_roots = sorted(all_roots)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for root in sorted_roots:\n",
    "        out.write(root + \"\\n\")\n",
    "\n",
    "print(f\"Done! Collected {len(sorted_roots)} possible roots.\")\n",
    "print(f\"Results saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: joblib in /Users/alibayram/Library/Python/3.9/lib/python/site-packages (1.4.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lines(lines_chunk):\n",
    "    \"\"\"\n",
    "    Process a chunk of lines in parallel:\n",
    "      1) Lowercase\n",
    "      2) Regex tokenization\n",
    "      3) Suffix-stripping\n",
    "      4) Return a local set of valid roots\n",
    "    \"\"\"\n",
    "    local_roots = set()\n",
    "    for line in lines_chunk:\n",
    "        line = line.lower()\n",
    "        tokens = word_pattern.findall(line)\n",
    "        for token in tokens:\n",
    "            if len(token) < 2:\n",
    "                continue\n",
    "            root_candidates = strip_suffixes(token, ekler, known_roots)\n",
    "            local_roots.update(root_candidates)\n",
    "    return local_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "n_jobs = 800\n",
    "\n",
    "chunk_size = len(all_lines) // n_jobs\n",
    "chunks = [all_lines[i:i+chunk_size] for i in range(0, len(all_lines), chunk_size)]\n",
    "\n",
    "print(f\"Split the data into {len(chunks)} chunks.\")\n",
    "\n",
    "\n",
    "# 6) Parallelize over chunks\n",
    "results = Parallel(n_jobs=n_jobs, verbose=5)(\n",
    "    delayed(process_lines)(chunk) for chunk in chunks\n",
    ")\n",
    "\n",
    "# 7) Combine (union) the resulting root sets\n",
    "all_roots = set()\n",
    "for res_set in results:\n",
    "    all_roots |= res_set\n",
    "\n",
    "print(f\"Collected total of {len(all_roots)} possible roots.\")\n",
    "\n",
    "# 8) Write them out\n",
    "output_file = \"possible_roots.txt\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    for root in sorted(all_roots):\n",
    "        out.write(root + \"\\n\")\n",
    "\n",
    "print(f\"Done. Results saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TURKISH_SUFFIXES = [\n",
    "    \"lar\", \"ler\",\n",
    "    # Common case endings\n",
    "    \"ı\", \"i\", \"u\", \"ü\",\n",
    "    \"yı\", \"yi\", \"yu\", \"yü\",\n",
    "    \"a\", \"e\",\n",
    "    \"ya\", \"ye\",\n",
    "    \"da\", \"de\", \"ta\", \"te\",\n",
    "    \"dan\", \"den\", \"tan\", \"ten\",\n",
    "    # Possessive\n",
    "    \"ım\", \"im\", \"um\", \"üm\",\n",
    "    \"nın\", \"nin\", \"nun\", \"nün\",\n",
    "    \"ımız\", \"imiz\", \"umuz\", \"ümüz\",\n",
    "    \"nız\", \"niz\", \"nuz\", \"nüz\",\n",
    "    # Verb tenses / person\n",
    "    \"m\", \"n\", \"k\", \"ız\", \"iz\", \"uz\", \"üz\",\n",
    "    \"dı\", \"di\", \"du\", \"dü\", \"tı\", \"ti\", \"tu\", \"tü\",\n",
    "    \"mış\", \"miş\", \"muş\", \"müş\",\n",
    "    \"acak\", \"ecek\",\n",
    "    \"ar\", \"er\",\n",
    "    # Some derivational\n",
    "    \"lık\", \"lik\", \"luk\", \"lük\",\n",
    "    \"cı\", \"ci\", \"cu\", \"cü\",\n",
    "    \"çı\", \"çi\", \"çu\", \"çü\",\n",
    "    \"laş\", \"leş\", \"laşma\", \"leşme\",\n",
    "    \"ma\", \"me\", \"ış\", \"iş\", \"uş\", \"üş\",\n",
    "    # ... add more if needed\n",
    "]\n",
    "\n",
    "TURKISH_SUFFIXES = set(TURKISH_SUFFIXES)\n",
    "for ek in ekler:\n",
    "    TURKISH_SUFFIXES.add(ek)\n",
    "\n",
    "len(TURKISH_SUFFIXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex for extracting words (just Turkish letters)\n",
    "WORD_PATTERN = re.compile(r\"[abcçdefgğhıijklmnoöprsştuüvyz]+\", re.IGNORECASE)\n",
    "\n",
    "def strip_all_suffixes(word, suffixes):\n",
    "    \"\"\"\n",
    "    Recursively strip known Turkish suffixes from 'word'.\n",
    "    Returns a set of *all* possible stripped forms (including 'word' itself).\n",
    "    \n",
    "    We do *not* filter by a known-roots list here; we want to discover *all* possibilities.\n",
    "    \"\"\"\n",
    "    # Local memo to avoid recomputing for the same form within a single call\n",
    "    local_memo = {}\n",
    "    \n",
    "    def _recur_strip(w):\n",
    "        if w in local_memo:\n",
    "            return local_memo[w]\n",
    "        \n",
    "        candidates = {w}  # always include the current form\n",
    "        for sfx in suffixes:\n",
    "            if w.endswith(sfx) and len(w) > len(sfx):\n",
    "                truncated = w[:-len(sfx)]\n",
    "                subcands = _recur_strip(truncated)\n",
    "                candidates.update(subcands)\n",
    "        \n",
    "        local_memo[w] = candidates\n",
    "        return candidates\n",
    "    \n",
    "    return _recur_strip(word)\n",
    "\n",
    "def process_lines_chunk(lines_chunk):\n",
    "    \"\"\"\n",
    "    Process a chunk of lines in parallel:\n",
    "      1) Lowercase\n",
    "      2) Regex tokenization\n",
    "      3) Suffix-stripping\n",
    "      4) Return a local set of discovered roots (all possible forms)\n",
    "    \"\"\"\n",
    "    local_roots = set()\n",
    "    for line in lines_chunk:\n",
    "        line = line.lower()\n",
    "        tokens = WORD_PATTERN.findall(line)\n",
    "        for token in tokens:\n",
    "            if len(token) < 2:  # skip very short tokens\n",
    "                continue\n",
    "            possible_forms = strip_all_suffixes(token, TURKISH_SUFFIXES)\n",
    "            local_roots.update(possible_forms)\n",
    "    return local_roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10470 known roots from 'kokler.txt'.\n",
      "Split into 800 chunks for parallel processing.\n"
     ]
    }
   ],
   "source": [
    "# Jupyter Notebook Cell\n",
    "\n",
    "import math\n",
    "\n",
    "# 1) Load your 10k known Turkish roots\n",
    "known_roots_file = \"kokler.txt\"\n",
    "known_roots = set()\n",
    "with open(known_roots_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        root = line.strip().lower()\n",
    "        if root:\n",
    "            known_roots.add(root)\n",
    "\n",
    "print(f\"Loaded {len(known_roots)} known roots from '{known_roots_file}'.\")\n",
    "\n",
    "\n",
    "# 3) Split into chunks for parallel processing\n",
    "n_jobs = 800  # adjust based on your CPU cores\n",
    "chunk_size = math.ceil(len(all_lines) / n_jobs)\n",
    "chunks = []\n",
    "for i in range(0, len(all_lines), chunk_size):\n",
    "    chunks.append(all_lines[i:i+chunk_size])\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks for parallel processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=800)]: Using backend LokyBackend with 800 concurrent workers.\n",
      "[Parallel(n_jobs=800)]: Done   6 out of 800 | elapsed:  2.4min remaining: 321.2min\n",
      "[Parallel(n_jobs=800)]: Done 167 out of 800 | elapsed: 11.7min remaining: 44.5min\n",
      "[Parallel(n_jobs=800)]: Done 328 out of 800 | elapsed: 16.4min remaining: 23.6min\n",
      "[Parallel(n_jobs=800)]: Done 489 out of 800 | elapsed: 18.4min remaining: 11.7min\n",
      "[Parallel(n_jobs=800)]: Done 650 out of 800 | elapsed: 19.4min remaining:  4.5min\n",
      "[Parallel(n_jobs=800)]: Done 800 out of 800 | elapsed: 21.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 3955918 total possible root forms.\n",
      "  - 9816 forms match your known 10k list.\n",
      "  - 3946102 forms are *new* (not in your 10k list).\n",
      "Done!\n",
      "  - 'all_roots.txt' contains all discovered forms.\n",
      "  - 'found_in_known_roots.txt' are those also in your 10k list.\n",
      "  - 'newly_discovered_roots.txt' are those *not* in your 10k list.\n"
     ]
    }
   ],
   "source": [
    "# 4) Process each chunk in parallel\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "results = Parallel(n_jobs=n_jobs, verbose=5)(\n",
    "    delayed(process_lines_chunk)(chunk) for chunk in chunks\n",
    ")\n",
    "\n",
    "# 5) Combine all discovered root forms\n",
    "all_roots = set()\n",
    "for res in results:\n",
    "    all_roots |= res\n",
    "\n",
    "print(f\"Discovered {len(all_roots)} total possible root forms.\")\n",
    "\n",
    "# 6) Separate into 'found in known list' vs. 'newly discovered'\n",
    "known_roots_found = all_roots & known_roots\n",
    "new_roots = all_roots - known_roots\n",
    "\n",
    "print(f\"  - {len(known_roots_found)} forms match your known 10k list.\")\n",
    "print(f\"  - {len(new_roots)} forms are *new* (not in your 10k list).\")\n",
    "\n",
    "# 7) Save outputs\n",
    "with open(\"all_roots.txt\", \"w\", encoding=\"utf-8\") as f_all:\n",
    "    for root in sorted(all_roots):\n",
    "        f_all.write(root + \"\\n\")\n",
    "\n",
    "with open(\"found_in_known_roots.txt\", \"w\", encoding=\"utf-8\") as f_known:\n",
    "    for root in sorted(known_roots_found):\n",
    "        f_known.write(root + \"\\n\")\n",
    "\n",
    "with open(\"newly_discovered_roots.txt\", \"w\", encoding=\"utf-8\") as f_new:\n",
    "    for root in sorted(new_roots):\n",
    "        f_new.write(root + \"\\n\")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"  - 'all_roots.txt' contains all discovered forms.\")\n",
    "print(\"  - 'found_in_known_roots.txt' are those also in your 10k list.\")\n",
    "print(\"  - 'newly_discovered_roots.txt' are those *not* in your 10k list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 971022 word frequencies.\n",
      "the: 280629\n",
      "colspan: 148111\n",
      "d: 146231\n",
      "km: 96081\n",
      "i̇stanbul: 84672\n",
      "o: 79483\n",
      "i: 74743\n",
      "width: 72129\n",
      "osmanlı: 68964\n",
      "i̇ngiliz: 57315\n",
      "ii: 56943\n",
      "i̇lk: 55124\n",
      "fransız: 52252\n",
      "i̇yi: 48488\n",
      "e: 48127\n",
      "edildi: 46696\n",
      "m: 46078\n",
      "edilen: 44154\n",
      "rus: 39806\n",
      "fc: 39480\n",
      "ö: 39158\n",
      "i̇ngilizce: 38710\n",
      "b: 37731\n",
      "fransa: 34678\n",
      "edilmiştir: 34257\n",
      "oğlu: 30399\n",
      "to: 30044\n",
      "c: 29935\n",
      "s: 29349\n",
      "ngc: 27882\n",
      "tv: 27704\n",
      "i̇talyan: 26957\n",
      "mö: 26772\n",
      "sovyet: 26158\n",
      "mehmet: 25485\n",
      "x: 25425\n",
      "i̇kinci: 24946\n",
      "japon: 24817\n",
      "edilir: 24780\n",
      "rusya: 24594\n",
      "tbmm: 24231\n",
      "çin: 23998\n",
      "j: 23660\n",
      "i̇ki: 20178\n",
      "william: 19321\n",
      "edilmiş: 19101\n",
      "i̇talya: 18548\n",
      "rock: 18538\n",
      "v: 18400\n",
      "james: 18340\n",
      "paris: 18327\n",
      "sovyetler: 18296\n",
      "uefa: 18269\n",
      "robert: 18078\n",
      "van: 17934\n",
      "windows: 17882\n",
      "no: 17697\n",
      "i̇slam: 17622\n",
      "iii: 17408\n",
      "i̇mparatorluğu: 17320\n",
      "iken: 17241\n",
      "u: 17059\n",
      "f: 17024\n",
      "world: 17004\n",
      "civil: 16962\n",
      "i̇ngiltere: 16825\n",
      "dir: 16760\n",
      "g: 16535\n",
      "i̇zmir: 16494\n",
      "cm: 16436\n",
      "los: 15877\n",
      "von: 15632\n",
      "p: 15473\n",
      "george: 15318\n",
      "y: 15274\n",
      "i̇spanyol: 15098\n",
      "by: 14908\n",
      "t: 14730\n",
      "paul: 14692\n",
      "kg: 14512\n",
      "you: 14405\n",
      "kıbrıs: 14358\n",
      "i̇stanbul'da: 14293\n",
      "love: 14260\n",
      "with: 14254\n",
      "i̇spanya: 14127\n",
      "h: 14102\n",
      "i̇ran: 14082\n",
      "city: 14017\n",
      "l: 13737\n",
      "r: 13580\n",
      "charles: 13560\n",
      "hükûmet: 13172\n",
      "hükûmeti: 13109\n",
      "edilmektedir: 12823\n",
      "japonya: 12818\n",
      "k: 12624\n",
      "i̇srail: 12547\n",
      "from: 12491\n",
      "my: 12424\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"word_frequency_counter/remains_words_with_freq.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    freqs = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(freqs)} word frequencies.\")\n",
    "\n",
    "# sort by frequency\n",
    "sorted_freqs = sorted(freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print top 10\n",
    "for word, freq in sorted_freqs[:100]:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out to a new json utf-8 file\n",
    "sorted_freqs = dict(sorted_freqs)\n",
    "# remove if frequency is less than 100\n",
    "for word in list(sorted_freqs.keys()):\n",
    "    if len(word) < 2 or sorted_freqs[word] < 100:\n",
    "        del sorted_freqs[word]\n",
    "with open(\"./sorted_remains_words_freq.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted_freqs, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
