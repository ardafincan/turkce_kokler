{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekler = {\n",
    "    \"lak\": 0,\n",
    "    \"nın\": 1,\n",
    "    \"siniz\": 2,\n",
    "    \"ış\": 3,\n",
    "    \"ala\": 4,\n",
    "    \"mış\": 5,\n",
    "    \"ler\": 6,\n",
    "    \"de\": 7,\n",
    "    \"deki\": 8,\n",
    "    \"ka\": 9,\n",
    "    \"dür\": 10,\n",
    "    \"amak\": 11,\n",
    "    \"el\": 12,\n",
    "    \"cuk\": 13,\n",
    "    \"ır\": 14,\n",
    "    \"deş\": 15,\n",
    "    \"gıç\": 16,\n",
    "    \"teş\": 17,\n",
    "    \"eç\": 18,\n",
    "    \"ça\": 19,\n",
    "    \"ma\": 20,\n",
    "    \"imiz\": 21,\n",
    "    \"çıl\": 22,\n",
    "    \"enek\": 23,\n",
    "    \"gaç\": 24,\n",
    "    \"guç\": 25,\n",
    "    \"şer\": 26,\n",
    "    \"ga\": 27,\n",
    "    \"müş\": 28,\n",
    "    \"mek\": 29,\n",
    "    \"layın\": 30,\n",
    "    \"dır\": 31,\n",
    "    \"tan\": 32,\n",
    "    \"li\": 33,\n",
    "    \"ıntı\": 34,\n",
    "    \"anak\": 35,\n",
    "    \"y\": 36,\n",
    "    \"tı\": 37,\n",
    "    \"le\": 38,\n",
    "    \"mede\": 39,\n",
    "    \"nün\": 40,\n",
    "    \"kan\": 41,\n",
    "    \"yecek\": 42,\n",
    "    \"lü\": 43,\n",
    "    \"tır\": 44,\n",
    "    \"cul\": 45,\n",
    "    \"rek\": 46,\n",
    "    \"suz\": 47,\n",
    "    \"ncu\": 48,\n",
    "    \"kıl\": 49,\n",
    "    \"ecek\": 50,\n",
    "    \"mekte\": 51,\n",
    "    \"cik\": 52,\n",
    "    \"ce\": 53,\n",
    "    \"kur\": 54,\n",
    "    \"acak\": 55,\n",
    "    \"tür\": 56,\n",
    "    \"mık\": 57,\n",
    "    \"muk\": 58,\n",
    "    \"mu\": 59,\n",
    "    \"gül\": 60,\n",
    "    \"te\": 61,\n",
    "    \"çü\": 62,\n",
    "    \"meç\": 63,\n",
    "    \"nci\": 64,\n",
    "    \"sel\": 65,\n",
    "    \"ydi\": 66,\n",
    "    \"şin\": 67,\n",
    "    \"rak\": 68,\n",
    "    \"tü\": 69,\n",
    "    \"me\": 70,\n",
    "    \"ti\": 71,\n",
    "    \"leyin\": 72,\n",
    "    \"sı\": 73,\n",
    "    \"sal\": 74,\n",
    "    \"tu\": 75,\n",
    "    \"muş\": 76,\n",
    "    \"leri\": 77,\n",
    "    \"gı\": 78,\n",
    "    \"cü\": 79,\n",
    "    \"ek\": 80,\n",
    "    \"ar\": 81,\n",
    "    \"eceğ\": 82,\n",
    "    \"çuk\": 83,\n",
    "    \"ak\": 84,\n",
    "    \"dü\": 85,\n",
    "    \"den\": 86,\n",
    "    \"çı\": 87,\n",
    "    \"an\": 88,\n",
    "    \"alak\": 89,\n",
    "    \"ncı\": 90,\n",
    "    \"kür\": 91,\n",
    "    \"cu\": 92,\n",
    "    \"mi\": 93,\n",
    "    \"dir\": 94,\n",
    "    \"aç\": 95,\n",
    "    \"t\": 96,\n",
    "    \"cı\": 97,\n",
    "    \"geç\": 98,\n",
    "    \"cıl\": 99,\n",
    "    \"ca\": 100,\n",
    "    \"iniz\": 101,\n",
    "    \"miş\": 102,\n",
    "    \"ı\": 103,\n",
    "    \"da\": 104,\n",
    "    \"k\": 105,\n",
    "    \"ün\": 106,\n",
    "    \"mik\": 107,\n",
    "    \"l\": 108,\n",
    "    \"al\": 109,\n",
    "    \"çük\": 110,\n",
    "    \"luk\": 111,\n",
    "    \"dan\": 112,\n",
    "    \"dirler\": 113,\n",
    "    \"lik\": 114,\n",
    "    \"sek\": 115,\n",
    "    \"iz\": 116,\n",
    "    \"kir\": 117,\n",
    "    \"yse\": 118,\n",
    "    \"p\": 119,\n",
    "    \"cül\": 120,\n",
    "    \"cileyin\": 121,\n",
    "    \"mü\": 122,\n",
    "    \"taş\": 123,\n",
    "    \"çil\": 124,\n",
    "    \"u\": 125,\n",
    "    \"ımsa\": 126,\n",
    "    \"kır\": 127,\n",
    "    \"si\": 128,\n",
    "    \"r\": 129,\n",
    "    \"n\": 130,\n",
    "    \"sü\": 131,\n",
    "    \"lı\": 132,\n",
    "    \"m\": 133,\n",
    "    \"makta\": 134,\n",
    "    \"man\": 135,\n",
    "    \"üncü\": 136,\n",
    "    \"nç\": 137,\n",
    "    \"lık\": 138,\n",
    "    \"dı\": 139,\n",
    "    \"la\": 140,\n",
    "    \"mece\": 141,\n",
    "    \"ten\": 142,\n",
    "    \"sin\": 143,\n",
    "    \"ele\": 144,\n",
    "    \"ın\": 145,\n",
    "    \"sa\": 146,\n",
    "    \"ayım\": 147,\n",
    "    \"çi\": 148,\n",
    "    \"eğen\": 149,\n",
    "    \"ge\": 150,\n",
    "    \"msi\": 151,\n",
    "    \"tur\": 152,\n",
    "    \"ken\": 153,\n",
    "    \"lek\": 154,\n",
    "    \"mse\": 155,\n",
    "    \"lar\": 156,\n",
    "    \"tir\": 157,\n",
    "    \"malı\": 158,\n",
    "    \"lu\": 159,\n",
    "    \"sız\": 160,\n",
    "    \"i\": 161,\n",
    "    \"süz\": 162,\n",
    "    \"se\": 163,\n",
    "    \"msı\": 164,\n",
    "    \"a\": 165,\n",
    "    \"mtırak\": 166,\n",
    "    \"daş\": 167,\n",
    "    \"ncü\": 168,\n",
    "    \"msu\": 169,\n",
    "    \"e\": 170,\n",
    "    \"çık\": 171,\n",
    "    \"men\": 172,\n",
    "    \"ç\": 173,\n",
    "    \"meli\": 174,\n",
    "    \"gil\": 175,\n",
    "    \"çul\": 176,\n",
    "    \"cil\": 177,\n",
    "    \"emek\": 178,\n",
    "    \"maca\": 179,\n",
    "    \"z\": 180,\n",
    "    \"sak\": 181,\n",
    "    \"du\": 182,\n",
    "    \"acağ\": 183,\n",
    "    \"kil\": 184,\n",
    "    \"di\": 185,\n",
    "    \"ş\": 186,\n",
    "    \"msa\": 187,\n",
    "    \"en\": 188,\n",
    "    \"ağan\": 189,\n",
    "    \"mak\": 190,\n",
    "    \"ıcı\": 191,\n",
    "    \"siz\": 192,\n",
    "    \"şar\": 193,\n",
    "    \"elek\": 194,\n",
    "    \"giç\": 195,\n",
    "    \"ki\": 196,\n",
    "    \"yacak\": 197,\n",
    "    \"çe\": 198,\n",
    "    \"şın\": 199,\n",
    "    \"kaç\": 200,\n",
    "    \"maç\": 201,\n",
    "    \"in\": 202,\n",
    "    \"ü\": 203,\n",
    "    \"nin\": 204,\n",
    "    \"ık\": 205,\n",
    "    \"kek\": 206,\n",
    "    \"cük\": 207,\n",
    "    \"im\": 208,\n",
    "    \"laş\": 209,\n",
    "    \"yor\": 210,\n",
    "    \"mı\": 211,\n",
    "    \"ım\": 212,\n",
    "    \"su\": 213,\n",
    "    \"lük\": 214,\n",
    "    \"çu\": 215,\n",
    "    \"dur\": 216,\n",
    "    \"un\": 217,\n",
    "    \"ci\": 218,\n",
    "    \"ymiş\": 219,\n",
    "    \"er\": 220,\n",
    "    \"cık\": 221,\n",
    "    \"çik\": 222,\n",
    "    \"ta\": 223,\n",
    "    \"nun\": 224\n",
    "}\n",
    "\n",
    "ekler = ekler.keys()\n",
    "\n",
    "len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TURKISH_SUFFIXES = [\n",
    "    \"lar\", \"ler\",\n",
    "    # Common case endings\n",
    "    \"ı\", \"i\", \"u\", \"ü\",\n",
    "    \"yı\", \"yi\", \"yu\", \"yü\",\n",
    "    \"a\", \"e\",\n",
    "    \"ya\", \"ye\",\n",
    "    \"da\", \"de\", \"ta\", \"te\",\n",
    "    \"dan\", \"den\", \"tan\", \"ten\",\n",
    "    # Possessive\n",
    "    \"ım\", \"im\", \"um\", \"üm\",\n",
    "    \"nın\", \"nin\", \"nun\", \"nün\",\n",
    "    \"ımız\", \"imiz\", \"umuz\", \"ümüz\",\n",
    "    \"nız\", \"niz\", \"nuz\", \"nüz\",\n",
    "    # Verb tenses / person\n",
    "    \"m\", \"n\", \"k\", \"ız\", \"iz\", \"uz\", \"üz\",\n",
    "    \"dı\", \"di\", \"du\", \"dü\", \"tı\", \"ti\", \"tu\", \"tü\",\n",
    "    \"mış\", \"miş\", \"muş\", \"müş\",\n",
    "    \"acak\", \"ecek\",\n",
    "    \"ar\", \"er\",\n",
    "    # Some derivational\n",
    "    \"lık\", \"lik\", \"luk\", \"lük\",\n",
    "    \"cı\", \"ci\", \"cu\", \"cü\",\n",
    "    \"çı\", \"çi\", \"çu\", \"çü\",\n",
    "    \"laş\", \"leş\", \"laşma\", \"leşme\",\n",
    "    \"ma\", \"me\", \"ış\", \"iş\", \"uş\", \"üş\",\n",
    "    # ... add more if needed\n",
    "]\n",
    "\n",
    "TURKISH_SUFFIXES = set(TURKISH_SUFFIXES)\n",
    "for ek in ekler:\n",
    "    TURKISH_SUFFIXES.add(ek)\n",
    "\n",
    "len(TURKISH_SUFFIXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 47it [10:39, 13.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dönüşümü sırasında hata oluştu: Expecting value: line 23 column 1 (char 251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 57it [13:00, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dönüşümü sırasında hata oluştu: Expecting value: line 19 column 1 (char 177)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 73it [17:15, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dönüşümü sırasında hata oluştu: Expecting value: line 74 column 1 (char 706)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 91it [22:07, 17.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dönüşümü sırasında hata oluştu: Expecting value: line 24 column 1 (char 253)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 98it [24:22, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON dönüşümü sırasında hata oluştu: Expecting value: line 23 column 1 (char 206)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 108it [26:06, 14.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm kökler başarıyla kaydedildi!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from ollama import chat\n",
    "\n",
    "# Elinizdeki kelime frekans listesinin JSON formatındaki örneği\n",
    "with open(\"sorted_remains_words_freq.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word_frequencies = json.load(f)\n",
    "\n",
    "# Sadece kelimeleri almak\n",
    "words = list(word_frequencies.keys())\n",
    "\n",
    "# 100'er gruplara bölme\n",
    "def chunk_list(data, chunk_size=100):\n",
    "    \"\"\"Bir listeyi belirtilen boyutta parçalara böler.\"\"\"\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        yield data[i:i + chunk_size]\n",
    "\n",
    "# Ollama'dan kökleri alma\n",
    "def get_roots_from_ollama(words_chunk, suffixes):\n",
    "    \"\"\"\n",
    "    Ollama modeline bir grup kelime gönderip köklerini alır.\n",
    "    \"\"\"\n",
    "    # Sistem promptu\n",
    "    system_prompt = f\"\"\"\n",
    "    Sen bir Türkçe dil kök analizi yapan uzman bir yardımcı araçsın. Görevin, verilen kelimelerden yalnızca Türkçe olanların köklerini bulmaktır. \n",
    "    Kök bulma işlemini yaparken şu kurallara dikkat et:\n",
    "\n",
    "    1. **Türkçe ekleri çıkararak yalnızca kelimenin kökünü bul**. Yapım ve çekim eklerini tamamen kaldır.\n",
    "    2. Türkçe'de yaygın kullanılan yabancı kökenli kelimeleri Türkçe kabul edebilirsin (örneğin: \"tren\", \"televizyon\").\n",
    "    3. Türkçe olmayan kelimeleri tamamen görmezden gel ve çıktıya dahil etme.\n",
    "    4. Kısaltmaları olduğu gibi bırak, uzun hallerini yazma veya değiştirme.\n",
    "    5. Çıktıyı yalnızca JSON formatında bir liste olarak ver. Her kök yalnızca bir kez yer almalıdır.\n",
    "\n",
    "    ### Ekler:\n",
    "    Türkçe'deki yapım ve çekim ekleri şunlardır:\n",
    "    {suffixes}\n",
    "\n",
    "    Örnek çıktı formatı:\n",
    "    [\n",
    "      \"kök1\",\n",
    "      \"kök2\",\n",
    "      \"kök3\"\n",
    "    ]\n",
    "\n",
    "    Aşağıdaki kelimelerin köklerini bul ve yalnızca JSON formatında bir liste ver. Eğer verilen kelimelerin hiçbiri Türkçe değilse, JSON formatında boş bir liste döndür:\n",
    "    []\n",
    "    \"\"\"\n",
    "\n",
    "    # Kullanıcı promptu\n",
    "    user_prompt = (\n",
    "        f\"Aşağıdaki kelimelerin sadece Türkçe olanlarının Türkçe köklerini listele:\\n\\n\"\n",
    "        f\"{', '.join(words_chunk)}\\n\\n\"\n",
    "        f\"Lütfen başka hiçbir şey yazma ve sadece JSON formatında cevap ver.\"\n",
    "    )\n",
    "\n",
    "    # Kullanıcı promptu\n",
    "    user_prompt = (\n",
    "        f\"Aşağıdaki kelimelerin sadece Türkçe olanlarının Türkçe köklerini listele:\\n\\n\"\n",
    "        f\"{', '.join(words_chunk)}\\n\\n\"\n",
    "        f\"Lütfen başka hiçbir şey yazma ve sadece JSON formatında cevap ver.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Ollama'ya sorgu gönder\n",
    "        response = chat(model=\"gemma2:27b\", messages=[\n",
    "            {'role': 'system', 'content': system_prompt},  # Sistem promptu\n",
    "            {'role': 'user', 'content': user_prompt},      # Kullanıcı promptu\n",
    "        ])\n",
    "\n",
    "        # Dönen yanıt\n",
    "        return response.message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Hata oluştu: {e}\")\n",
    "        return None\n",
    "\n",
    "# Tüm kelimeleri 100'erli gruplar halinde işle\n",
    "for chunk in tqdm(chunk_list(words, chunk_size=100), desc=\"Processing chunks\"):\n",
    "    response = get_roots_from_ollama(chunk, TURKISH_SUFFIXES)\n",
    "    if response:\n",
    "        try:\n",
    "            # JSON yanıtını Python listesine dönüştür\n",
    "            response = json.loads(response)\n",
    "\n",
    "            # Her bir chunk'ın sonuçlarını ayrı bir dosyaya kaydet\n",
    "            file_name = f\"roots/r_{chunk[0]}_{chunk[-1]}.json\"\n",
    "            with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(response, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON dönüşümü sırasında hata oluştu: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"Tüm kökler başarıyla kaydedildi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tüm kökler başarıyla birleştirildi ve kaydedildi! 3662\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Tüm kök dosyalarını birleştir\n",
    "all_roots = set()\n",
    "for root_file in os.listdir(\"roots\"):\n",
    "    with open(f\"roots/{root_file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "        roots = json.load(f)\n",
    "        all_roots.update(roots)\n",
    "\n",
    "# Birleştirilmiş kökleri bir dosyaya kaydet\n",
    "with open(\"all_roots.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(list(all_roots), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Tüm kökler başarıyla birleştirildi ve kaydedildi!\", len(all_roots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12007"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kokler = set()\n",
    "\n",
    "# read all files and add lemmas to kokler set\n",
    "for file in files:\n",
    "    with open ('upos_lemmas/' + file, 'r') as f:\n",
    "        for line in f:\n",
    "            kokler.add(line.strip())\n",
    "\n",
    "len(kokler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10491, 25730)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kokler), len(kokler3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25730"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kokler3 = set()\n",
    "with open('kokler3.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "          kokler3.add(line.strip())\n",
    "\n",
    "# possible_kokler = possible_kokler + kokler\n",
    "for kok in kokler:\n",
    "    kokler3.add(kok)\n",
    "\n",
    "len(kokler3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "vowel = 'ıiuü'\n",
    "\n",
    "# check if a word finishes with l + vowel and kokler contains the word without l + vowel\n",
    "def check(word):\n",
    "    if len(word) > 4 and word[-2] == 'l' and word[-1] in vowel:\n",
    "        if word[:-2] in kokler:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# check if a word finishes with l + vowel + k and kokler contains the word without l + vowel + k\n",
    "\n",
    "def check2(word):\n",
    "    if len(word) > 5 and word[-3] == 'c' and word[-2] in vowel and word[-1] == 'k':\n",
    "        if word[:-3] in kokler:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "ekli = set()\n",
    "# check all words in kokler set\n",
    "for kok in kokler3:\n",
    "    if check(kok):\n",
    "        ekli.add(kok)\n",
    "\n",
    "print(len(ekli))\n",
    "# write ekli set to a file\n",
    "with open('ekli.txt', 'w') as f:\n",
    "    for ek in ekli:\n",
    "        f.write(ek + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9804\n"
     ]
    }
   ],
   "source": [
    "# read ekli words from a file\n",
    "with open('ekli.txt', 'r') as f:\n",
    "    ekli = set(f.read().split('\\n'))\n",
    "\n",
    "# remove ekli words from kokler set\n",
    "for ek in ekli:\n",
    "    if ek in kokler3:\n",
    "      kokler3.remove(ek)\n",
    "\n",
    "print(len(kokler3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9804"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('kokler3.txt', 'w') as f:\n",
    "    for kok in kokler3:\n",
    "        if len(kok) > 1:\n",
    "          f.write(kok + '\\n')\n",
    "len(kokler3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9805"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read kokler words from a file\n",
    "with open('possible_kokler.txt', 'r') as f:\n",
    "    possible_kokler = set(f.read().split('\\n'))\n",
    "len(possible_kokler)\n",
    "# check if a word finishes with l + vowel and kokler contains the word without l + vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10491"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read kokler words from a file\n",
    "with open('kokler.txt', 'r') as f:\n",
    "    kokler = set(f.read().split('\\n'))\n",
    "len(kokler)\n",
    "# check if a word finishes with l + vowel and kokler contains the word without l + vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10491"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make first letter lowercase\n",
    "def lower(word):\n",
    "    if len(word) == 0:\n",
    "        return ''\n",
    "    return word[0].lower() + word[1:]\n",
    "\n",
    "kokler = set(map(lower, kokler))\n",
    "\n",
    "len(kokler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
