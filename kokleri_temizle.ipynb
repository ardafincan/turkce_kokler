{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kokler = set()\n",
    "with open('kokler4.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if len(line.strip()) > 0:\n",
    "          kokler.add(line.strip())\n",
    "\n",
    "# possible_kokler = possible_kokler + kokler\n",
    "for kok in kokler:\n",
    "    kokler.add(kok)\n",
    "\n",
    "len(kokler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21109"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove 'giller' suffix from kokler\n",
    "temp_kokler = kokler.copy()\n",
    "for kok in temp_kokler:\n",
    "    if len(kok) > 7 and kok.endswith('giller'):\n",
    "        kokler.remove(kok)\n",
    "        kokler.add(kok[:-7])\n",
    "\n",
    "kokler = sorted(kokler)\n",
    "\n",
    "with open('kokler5.txt', 'w') as f:\n",
    "    for kok in kokler:\n",
    "        f.write(kok + '\\n')\n",
    "\n",
    "len(kokler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "vowel = 'ıiuü'\n",
    "constants = 'lcç'\n",
    "# check if a word finishes with l + vowel and kokler contains the word without l + vowel\n",
    "def check(word):\n",
    "    if len(word) > 4 and word[-2] == 'l' and word[-1] in vowel:\n",
    "        if word[:-2] in kokler:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# check if a word finishes with l + vowel + k and kokler contains the word without l + vowel + k\n",
    "\n",
    "def check2(word):\n",
    "    if len(word) > 5 and word[-3] == 'c' and word[-2] in vowel and word[-1] == 'k':\n",
    "        if word[:-3] in kokler:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "ekli = set()\n",
    "# check all words in kokler set\n",
    "for kok in kokler:\n",
    "    if check(kok):\n",
    "        ekli.add(kok)\n",
    "\n",
    "print(len(ekli))\n",
    "# write ekli set to a file\n",
    "with open('ekli.txt', 'w') as f:\n",
    "    for ek in ekli:\n",
    "        f.write(ek + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21159\n"
     ]
    }
   ],
   "source": [
    "# read ekli words from a file\n",
    "with open('ekli3.txt', 'r') as f:\n",
    "    ekli = set(f.read().split('\\n'))\n",
    "\n",
    "# remove ekli words from kokler set\n",
    "for ek in ekli:\n",
    "    if ek in kokler:\n",
    "      kokler.remove(ek)\n",
    "\n",
    "print(len(kokler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21159"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort kokler set and write to a file\n",
    "kokler = sorted(kokler)\n",
    "with open('kokler4.txt', 'w') as f:\n",
    "    for kok in kokler:\n",
    "        if len(kok) > 1:\n",
    "          f.write(kok + '\\n')\n",
    "len(kokler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ekler = {\n",
    "    \"deki\": 8,\n",
    "    \"cuk\": 13,\n",
    "    \"ır\": 14,\n",
    "    \"deş\": 15,\n",
    "    \"gıç\": 16,\n",
    "    \"teş\": 17,\n",
    "    \"eç\": 18,\n",
    "    \"ça\": 19,\n",
    "    \"ma\": 20,\n",
    "    \"imiz\": 21,\n",
    "    \"çıl\": 22,\n",
    "    \"enek\": 23,\n",
    "    \"gaç\": 24,\n",
    "    \"guç\": 25,\n",
    "    \"şer\": 26,\n",
    "    \"ga\": 27,\n",
    "    \"müş\": 28,\n",
    "    \"mek\": 29,\n",
    "    \"layın\": 30,\n",
    "    \"dır\": 31,\n",
    "    \"tan\": 32,\n",
    "    \"li\": 33,\n",
    "    \"ıntı\": 34,\n",
    "    \"anak\": 35,\n",
    "    \"y\": 36,\n",
    "    \"tı\": 37,\n",
    "    \"le\": 38,\n",
    "    \"mede\": 39,\n",
    "    \"nün\": 40,\n",
    "    \"kan\": 41,\n",
    "    \"yecek\": 42,\n",
    "    \"lü\": 43,\n",
    "    \"tır\": 44,\n",
    "    \"cul\": 45,\n",
    "    \"rek\": 46,\n",
    "    \"suz\": 47,\n",
    "    \"ncu\": 48,\n",
    "    \"kıl\": 49,\n",
    "    \"ecek\": 50,\n",
    "    \"mekte\": 51,\n",
    "    \"cik\": 52,\n",
    "    \"ce\": 53,\n",
    "    \"kur\": 54,\n",
    "    \"acak\": 55,\n",
    "    \"tür\": 56,\n",
    "    \"mık\": 57,\n",
    "    \"muk\": 58,\n",
    "    \"mu\": 59,\n",
    "    \"gül\": 60,\n",
    "    \"te\": 61,\n",
    "    \"çü\": 62,\n",
    "    \"meç\": 63,\n",
    "    \"nci\": 64,\n",
    "    \"sel\": 65,\n",
    "    \"ydi\": 66,\n",
    "    \"şin\": 67,\n",
    "    \"rak\": 68,\n",
    "    \"tü\": 69,\n",
    "    \"me\": 70,\n",
    "    \"ti\": 71,\n",
    "    \"leyin\": 72,\n",
    "    \"sı\": 73,\n",
    "    \"sal\": 74,\n",
    "    \"tu\": 75,\n",
    "    \"muş\": 76,\n",
    "    \"leri\": 77,\n",
    "    \"gı\": 78,\n",
    "    \"cü\": 79,\n",
    "    \"ek\": 80,\n",
    "    \"ar\": 81,\n",
    "    \"eceğ\": 82,\n",
    "    \"çuk\": 83,\n",
    "    \"ak\": 84,\n",
    "    \"dü\": 85,\n",
    "    \"den\": 86,\n",
    "    \"çı\": 87,\n",
    "    \"an\": 88,\n",
    "    \"alak\": 89,\n",
    "    \"ncı\": 90,\n",
    "    \"kür\": 91,\n",
    "    \"cu\": 92,\n",
    "    \"mi\": 93,\n",
    "    \"dir\": 94,\n",
    "    \"aç\": 95,\n",
    "    \"t\": 96,\n",
    "    \"cı\": 97,\n",
    "    \"geç\": 98,\n",
    "    \"cıl\": 99,\n",
    "    \"ca\": 100,\n",
    "    \"iniz\": 101,\n",
    "    \"miş\": 102,\n",
    "    \"ı\": 103,\n",
    "    \"da\": 104,\n",
    "    \"k\": 105,\n",
    "    \"ün\": 106,\n",
    "    \"mik\": 107,\n",
    "    \"l\": 108,\n",
    "    \"al\": 109,\n",
    "    \"çük\": 110,\n",
    "    \"luk\": 111,\n",
    "    \"dan\": 112,\n",
    "    \"dirler\": 113,\n",
    "    \"lik\": 114,\n",
    "    \"sek\": 115,\n",
    "    \"iz\": 116,\n",
    "    \"kir\": 117,\n",
    "    \"yse\": 118,\n",
    "    \"p\": 119,\n",
    "    \"cül\": 120,\n",
    "    \"cileyin\": 121,\n",
    "    \"mü\": 122,\n",
    "    \"taş\": 123,\n",
    "    \"çil\": 124,\n",
    "    \"u\": 125,\n",
    "    \"ımsa\": 126,\n",
    "    \"kır\": 127,\n",
    "    \"si\": 128,\n",
    "    \"r\": 129,\n",
    "    \"n\": 130,\n",
    "    \"sü\": 131,\n",
    "    \"lı\": 132,\n",
    "    \"m\": 133,\n",
    "    \"makta\": 134,\n",
    "    \"man\": 135,\n",
    "    \"üncü\": 136,\n",
    "    \"nç\": 137,\n",
    "    \"lık\": 138,\n",
    "    \"dı\": 139,\n",
    "    \"la\": 140,\n",
    "    \"mece\": 141,\n",
    "    \"ten\": 142,\n",
    "    \"sin\": 143,\n",
    "    \"ele\": 144,\n",
    "    \"ın\": 145,\n",
    "    \"sa\": 146,\n",
    "    \"ayım\": 147,\n",
    "    \"çi\": 148,\n",
    "    \"eğen\": 149,\n",
    "    \"ge\": 150,\n",
    "    \"msi\": 151,\n",
    "    \"tur\": 152,\n",
    "    \"ken\": 153,\n",
    "    \"lek\": 154,\n",
    "    \"mse\": 155,\n",
    "    \"lar\": 156,\n",
    "    \"tir\": 157,\n",
    "    \"malı\": 158,\n",
    "    \"lu\": 159,\n",
    "    \"sız\": 160,\n",
    "    \"i\": 161,\n",
    "    \"süz\": 162,\n",
    "    \"se\": 163,\n",
    "    \"msı\": 164,\n",
    "    \"a\": 165,\n",
    "    \"mtırak\": 166,\n",
    "    \"daş\": 167,\n",
    "    \"ncü\": 168,\n",
    "    \"msu\": 169,\n",
    "    \"e\": 170,\n",
    "    \"çık\": 171,\n",
    "    \"men\": 172,\n",
    "    \"ç\": 173,\n",
    "    \"meli\": 174,\n",
    "    \"gil\": 175,\n",
    "    \"çul\": 176,\n",
    "    \"cil\": 177,\n",
    "    \"emek\": 178,\n",
    "    \"maca\": 179,\n",
    "    \"z\": 180,\n",
    "    \"sak\": 181,\n",
    "    \"du\": 182,\n",
    "    \"acağ\": 183,\n",
    "    \"kil\": 184,\n",
    "    \"di\": 185,\n",
    "    \"ş\": 186,\n",
    "    \"msa\": 187,\n",
    "    \"en\": 188,\n",
    "    \"ağan\": 189,\n",
    "    \"mak\": 190,\n",
    "    \"ıcı\": 191,\n",
    "    \"siz\": 192,\n",
    "    \"şar\": 193,\n",
    "    \"elek\": 194,\n",
    "    \"giç\": 195,\n",
    "    \"ki\": 196,\n",
    "    \"yacak\": 197,\n",
    "    \"çe\": 198,\n",
    "    \"şın\": 199,\n",
    "    \"kaç\": 200,\n",
    "    \"maç\": 201,\n",
    "    \"in\": 202,\n",
    "    \"ü\": 203,\n",
    "    \"nin\": 204,\n",
    "    \"ık\": 205,\n",
    "    \"kek\": 206,\n",
    "    \"cük\": 207,\n",
    "    \"im\": 208,\n",
    "    \"laş\": 209,\n",
    "    \"yor\": 210,\n",
    "    \"mı\": 211,\n",
    "    \"ım\": 212,\n",
    "    \"su\": 213,\n",
    "    \"lük\": 214,\n",
    "    \"çu\": 215,\n",
    "    \"dur\": 216,\n",
    "    \"un\": 217,\n",
    "    \"ci\": 218,\n",
    "    \"ymiş\": 219,\n",
    "    \"er\": 220,\n",
    "    \"cık\": 221,\n",
    "    \"çik\": 222,\n",
    "    \"ta\": 223,\n",
    "    \"nun\": 224\n",
    "}\n",
    "\n",
    "ekler = ekler.keys()\n",
    "\n",
    "len(ekler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TURKISH_SUFFIXES = [\n",
    "    \"lar\", \"ler\",\n",
    "    # Common case endings\n",
    "    \"ı\", \"i\", \"u\", \"ü\",\n",
    "    \"yı\", \"yi\", \"yu\", \"yü\",\n",
    "    \"a\", \"e\",\n",
    "    \"ya\", \"ye\",\n",
    "    \"da\", \"de\", \"ta\", \"te\",\n",
    "    \"dan\", \"den\", \"tan\", \"ten\",\n",
    "    # Possessive\n",
    "    \"ım\", \"im\", \"um\", \"üm\",\n",
    "    \"nın\", \"nin\", \"nun\", \"nün\",\n",
    "    \"ımız\", \"imiz\", \"umuz\", \"ümüz\",\n",
    "    \"nız\", \"niz\", \"nuz\", \"nüz\",\n",
    "    # Verb tenses / person\n",
    "    \"m\", \"n\", \"k\", \"ız\", \"iz\", \"uz\", \"üz\",\n",
    "    \"dı\", \"di\", \"du\", \"dü\", \"tı\", \"ti\", \"tu\", \"tü\",\n",
    "    \"mış\", \"miş\", \"muş\", \"müş\",\n",
    "    \"acak\", \"ecek\",\n",
    "    \"ar\", \"er\",\n",
    "    # Some derivational\n",
    "    \"lık\", \"lik\", \"luk\", \"lük\",\n",
    "    \"cı\", \"ci\", \"cu\", \"cü\",\n",
    "    \"çı\", \"çi\", \"çu\", \"çü\",\n",
    "    \"laş\", \"leş\", \"laşma\", \"leşme\",\n",
    "    \"ma\", \"me\", \"ış\", \"iş\", \"uş\", \"üş\",\n",
    "    # ... add more if needed\n",
    "]\n",
    "\n",
    "TURKISH_SUFFIXES = set(TURKISH_SUFFIXES)\n",
    "for ek in ekler:\n",
    "    TURKISH_SUFFIXES.add(ek)\n",
    "\n",
    "len(TURKISH_SUFFIXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4763\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "suffix_count = {}\n",
    "# check if a word finishes with a suffix in TURKISH_SUFFIXES set and kokler contains the word without the suffix\n",
    "def check3(word):\n",
    "    for suffix in TURKISH_SUFFIXES:\n",
    "        if len(suffix) < 2:\n",
    "            continue\n",
    "        if len(word) > len(suffix) and word.endswith(suffix):\n",
    "            if len(word[:-len(suffix)]) > 1 and word[:-len(suffix)] in kokler:\n",
    "                suffix_count[suffix] = suffix_count.get(suffix, 0) + 1\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "ekli = set()\n",
    "# check all words in kokler set\n",
    "for kok in kokler:\n",
    "    if check3(kok):\n",
    "        ekli.add(kok)\n",
    "\n",
    "# sort suffix_count dictionary by values\n",
    "suffix_count = dict(sorted(suffix_count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open('suffix_count.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(suffix_count, f, ensure_ascii=False)\n",
    "\n",
    "print(len(ekli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ekli2.txt', 'w') as f:\n",
    "    for ek in ekli:\n",
    "        f.write(ek + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438\n"
     ]
    }
   ],
   "source": [
    "vowel = 'ıiuü'\n",
    "constants = 'lcç'\n",
    "\n",
    "def check4(word):\n",
    "    if len(word) > 4 and word[-2] in constants and word[-1] in vowel:\n",
    "        if word[:-2] in kokler:\n",
    "            return True\n",
    "        \n",
    "    if len(word) > 5 and word[-3] in constants and word[-2] in vowel and word[-1] == 'k':\n",
    "        if word[:-3] in kokler:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "ekli = set()\n",
    "for kok in kokler:\n",
    "    if check4(kok):\n",
    "        ekli.add(kok)\n",
    "\n",
    "with open('ekli3.txt', 'w') as f:\n",
    "    for ek in ekli:\n",
    "        f.write(ek + '\\n')\n",
    "\n",
    "print(len(ekli))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14153"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# read files in roots2 directory\n",
    "# json files\n",
    "\n",
    "root_files = []\n",
    "\n",
    "for root, dirs, files in os.walk('roots'):\n",
    "    for file in files:\n",
    "        if file.endswith('.json'):\n",
    "            root_files.append(os.path.join(root, file))\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf8') as f:\n",
    "                try:\n",
    "                    roots = roots.union(set(json.load(f)))\n",
    "                    os.rename(os.path.join(root, file), os.path.join('processed/combined_roots', file))\n",
    "                except:\n",
    "                    print(file)\n",
    "\n",
    "len(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and save to a text file\n",
    "roots = sorted(roots)\n",
    "with open('combined_roots_v2.txt', 'w') as f:\n",
    "    for root in roots:\n",
    "        f.write(root + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21541, 1044)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read sorted_combined_roots_with_freq.json file\n",
    "with open('sorted_combined_roots_with_freq.json', 'r', encoding='utf8') as f:\n",
    "    combined_roots = json.load(f)\n",
    "\n",
    "combined_roots = set(combined_roots.keys())\n",
    "\n",
    "# make first letter of each root lowercase\n",
    "combined_roots = set([root[0].lower() + root[1:] for root in combined_roots])\n",
    "roots = set([root[0].lower() + root[1:] for root in roots])\n",
    "    \n",
    "len(combined_roots), len(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22029"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_roots.update(roots)\n",
    "# save combined_roots set to a text file\n",
    "with open('combined_roots.txt', 'w') as f:\n",
    "    for root in combined_roots:\n",
    "        f.write(root + '\\n')\n",
    "\n",
    "len(combined_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22029"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort with length desc and alphabetically\n",
    "combined_roots = sorted(combined_roots, key=lambda x: (-len(x), x))\n",
    "\n",
    "# save to a text file\n",
    "with open('desc_sorted_combined_roots.txt', 'w') as f:\n",
    "    for root in combined_roots:\n",
    "        f.write(root + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
